[2022-04-17 01:52:18,259][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:18,260][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:18,261][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 3
[2022-04-17 01:52:18,262][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:18,262][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:18,264][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+---------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name           | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+---------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 0.000000   | iteration_0.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.007261      | 6885.929143         | 688.592914           | 9.400000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+---------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:18,397][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:18,398][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:18,399][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 10.0, current episode: 3
[2022-04-17 01:52:18,400][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:18,401][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 11.0, current episode: 5
[2022-04-17 01:52:18,403][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name            | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 42.000000  | iteration_42.pth.tar | 5.000000      | 55.000000     | 11.000000               | 0.008150      | 6748.453149         | 613.495741           | 9.800000    | 0.748331   | 11.000000  | 9.000000   |
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:18,511][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:18,512][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:18,513][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 3
[2022-04-17 01:52:18,515][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:18,515][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:18,517][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name            | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 84.000000  | iteration_84.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.007594      | 6584.159140         | 658.415914           | 9.400000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:18,639][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:18,640][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:18,641][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 10.0, current episode: 3
[2022-04-17 01:52:18,642][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:18,643][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:18,644][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 126.000000 | iteration_126.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.007733      | 6465.534969         | 646.553497           | 9.600000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:18,775][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:18,776][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:18,777][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 3
[2022-04-17 01:52:18,778][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:18,779][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:18,781][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 168.000000 | iteration_168.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.009032      | 5536.009488         | 553.600949           | 9.400000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:18,908][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:18,910][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:18,911][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 3
[2022-04-17 01:52:18,912][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:18,913][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:18,915][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 210.000000 | iteration_210.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.009248      | 5406.799007         | 540.679901           | 9.400000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:19,042][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:19,043][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:19,044][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 3
[2022-04-17 01:52:19,046][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:19,047][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:19,048][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 252.000000 | iteration_252.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.009101      | 5493.713625         | 549.371362           | 9.400000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:19,155][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 13.0, current episode: 1
[2022-04-17 01:52:19,157][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 15.0, current episode: 2
[2022-04-17 01:52:19,158][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 16.0, current episode: 3
[2022-04-17 01:52:19,160][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 17.0, current episode: 4
[2022-04-17 01:52:19,161][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 17.0, current episode: 5
[2022-04-17 01:52:19,162][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 294.000000 | iteration_294.pth.tar | 5.000000      | 85.000000     | 17.000000               | 0.009969      | 8526.739576         | 501.572916           | 15.600000   | 1.496663   | 17.000000  | 13.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:19,280][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 11.0, current episode: 1
[2022-04-17 01:52:19,282][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 12.0, current episode: 2
[2022-04-17 01:52:19,284][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 14.0, current episode: 3
[2022-04-17 01:52:19,285][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 14.0, current episode: 4
[2022-04-17 01:52:19,286][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 14.0, current episode: 5
[2022-04-17 01:52:19,287][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 336.000000 | iteration_336.pth.tar | 5.000000      | 70.000000     | 14.000000               | 0.009386      | 7457.928424         | 532.709173           | 13.000000   | 1.264911   | 14.000000  | 11.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:19,395][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 20.0, current episode: 1
[2022-04-17 01:52:19,397][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 23.0, current episode: 2
[2022-04-17 01:52:19,399][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 26.0, current episode: 3
[2022-04-17 01:52:19,400][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 26.0, current episode: 4
[2022-04-17 01:52:19,401][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 28.0, current episode: 5
[2022-04-17 01:52:19,402][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 378.000000 | iteration_378.pth.tar | 5.000000      | 140.000000    | 28.000000               | 0.012327      | 11357.264564        | 405.616592           | 24.600000   | 2.800000   | 28.000000  | 20.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:19,590][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 27.0, current episode: 1
[2022-04-17 01:52:19,592][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 30.0, current episode: 2
[2022-04-17 01:52:19,593][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 32.0, current episode: 3
[2022-04-17 01:52:19,595][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 37.0, current episode: 4
[2022-04-17 01:52:19,598][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 43.0, current episode: 5
[2022-04-17 01:52:19,599][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 420.000000 | iteration_420.pth.tar | 5.000000      | 215.000000    | 43.000000               | 0.023351      | 9207.197959         | 214.120883           | 33.800000   | 5.635601   | 43.000000  | 27.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:19,732][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 20.0, current episode: 1
[2022-04-17 01:52:19,734][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 20.0, current episode: 2
[2022-04-17 01:52:19,735][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 21.0, current episode: 3
[2022-04-17 01:52:19,737][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 24.0, current episode: 4
[2022-04-17 01:52:19,740][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 29.0, current episode: 5
[2022-04-17 01:52:19,741][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 462.000000 | iteration_462.pth.tar | 5.000000      | 145.000000    | 29.000000               | 0.014714      | 9854.549593         | 339.812055           | 22.800000   | 3.429286   | 29.000000  | 20.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:19,848][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 13.0, current episode: 1
[2022-04-17 01:52:19,850][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 15.0, current episode: 2
[2022-04-17 01:52:19,850][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 15.0, current episode: 3
[2022-04-17 01:52:19,851][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 15.0, current episode: 4
[2022-04-17 01:52:19,853][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 16.0, current episode: 5
[2022-04-17 01:52:19,854][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 504.000000 | iteration_504.pth.tar | 5.000000      | 80.000000     | 16.000000               | 0.009163      | 8731.001197         | 545.687575           | 14.800000   | 0.979796   | 16.000000  | 13.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:20,105][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 200.0, current episode: 1
[2022-04-17 01:52:20,106][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 200.0, current episode: 2
[2022-04-17 01:52:20,107][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 200.0, current episode: 3
[2022-04-17 01:52:20,108][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 200.0, current episode: 4
[2022-04-17 01:52:20,108][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 200.0, current episode: 5
[2022-04-17 01:52:20,109][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 546.000000 | iteration_546.pth.tar | 5.000000      | 1000.000000   | 200.000000              | 0.051951      | 19249.034754        | 96.245174            | 200.000000  | 0.000000   | 200.000000 | 200.000000 |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:20,115][interaction_serial_evaluator.py][line: 245][    INFO] [DI-engine serial pipeline] Current eval_reward: 200.0 is greater than stop_value: 195, so your RL agent is converged, you can refer to 'log/evaluator/evaluator_logger.txt' for details.
[2022-04-17 01:52:26,952][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 200.0, current episode: 1
[2022-04-17 01:52:26,953][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 200.0, current episode: 2
[2022-04-17 01:52:26,954][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 200.0, current episode: 3
[2022-04-17 01:52:26,954][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 200.0, current episode: 4
[2022-04-17 01:52:26,955][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 200.0, current episode: 5
[2022-04-17 01:52:26,956][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 546.000000 | iteration_546.pth.tar | 5.000000      | 1000.000000   | 200.000000              | 5.525019      | 180.994867          | 0.904974             | 200.000000  | 0.000000   | 200.000000 | 200.000000 |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:26,963][interaction_serial_evaluator.py][line: 245][    INFO] [DI-engine serial pipeline] Current eval_reward: 200.0 is greater than stop_value: 195, so your RL agent is converged, you can refer to 'log/evaluator/evaluator_logger.txt' for details.
[2022-04-17 01:52:46,234][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:46,235][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:46,236][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 3
[2022-04-17 01:52:46,237][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:46,238][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:46,239][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+---------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name           | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+---------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 0.000000   | iteration_0.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.007301      | 6848.264559         | 684.826456           | 9.400000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+---------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:46,510][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:46,511][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:46,514][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 10.0, current episode: 3
[2022-04-17 01:52:46,515][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:46,517][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 11.0, current episode: 5
[2022-04-17 01:52:46,518][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name            | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 42.000000  | iteration_42.pth.tar | 5.000000      | 55.000000     | 11.000000               | 0.013869      | 3965.662912         | 360.514810           | 9.800000    | 0.748331   | 11.000000  | 9.000000   |
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:46,628][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:46,630][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:46,630][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 3
[2022-04-17 01:52:46,632][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:46,632][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:46,634][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name            | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 84.000000  | iteration_84.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.007650      | 6535.688130         | 653.568813           | 9.400000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:46,870][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:46,871][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:46,874][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 10.0, current episode: 3
[2022-04-17 01:52:46,875][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:46,876][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:46,878][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 126.000000 | iteration_126.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.013336      | 3749.376043         | 374.937604           | 9.600000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:47,005][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:47,006][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:47,007][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 3
[2022-04-17 01:52:47,008][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:47,009][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:47,011][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 168.000000 | iteration_168.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.009027      | 5539.130420         | 553.913042           | 9.400000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:47,192][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:47,193][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:47,194][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 3
[2022-04-17 01:52:47,196][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:47,197][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:47,199][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 210.000000 | iteration_210.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.013610      | 3673.773499         | 367.377350           | 9.400000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:47,328][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 9.0, current episode: 1
[2022-04-17 01:52:47,329][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 9.0, current episode: 2
[2022-04-17 01:52:47,330][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 9.0, current episode: 3
[2022-04-17 01:52:47,331][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 10.0, current episode: 4
[2022-04-17 01:52:47,332][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 10.0, current episode: 5
[2022-04-17 01:52:47,334][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 252.000000 | iteration_252.pth.tar | 5.000000      | 50.000000     | 10.000000               | 0.008994      | 5559.350473         | 555.935047           | 9.400000    | 0.489898   | 10.000000  | 9.000000   |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:47,440][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 13.0, current episode: 1
[2022-04-17 01:52:47,442][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 15.0, current episode: 2
[2022-04-17 01:52:47,443][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 16.0, current episode: 3
[2022-04-17 01:52:47,445][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 17.0, current episode: 4
[2022-04-17 01:52:47,446][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 17.0, current episode: 5
[2022-04-17 01:52:47,447][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 294.000000 | iteration_294.pth.tar | 5.000000      | 85.000000     | 17.000000               | 0.009841      | 8636.919643         | 508.054097           | 15.600000   | 1.496663   | 17.000000  | 13.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:47,564][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 11.0, current episode: 1
[2022-04-17 01:52:47,566][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 12.0, current episode: 2
[2022-04-17 01:52:47,568][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 14.0, current episode: 3
[2022-04-17 01:52:47,569][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 14.0, current episode: 4
[2022-04-17 01:52:47,569][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 14.0, current episode: 5
[2022-04-17 01:52:47,570][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 336.000000 | iteration_336.pth.tar | 5.000000      | 70.000000     | 14.000000               | 0.008814      | 7942.299768         | 567.307126           | 13.000000   | 1.264911   | 14.000000  | 11.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:47,681][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 20.0, current episode: 1
[2022-04-17 01:52:47,684][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 23.0, current episode: 2
[2022-04-17 01:52:47,686][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 26.0, current episode: 3
[2022-04-17 01:52:47,686][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 26.0, current episode: 4
[2022-04-17 01:52:47,688][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 28.0, current episode: 5
[2022-04-17 01:52:47,689][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 378.000000 | iteration_378.pth.tar | 5.000000      | 140.000000    | 28.000000               | 0.012429      | 11263.842074        | 402.280074           | 24.600000   | 2.800000   | 28.000000  | 20.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:47,832][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 27.0, current episode: 1
[2022-04-17 01:52:47,835][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 30.0, current episode: 2
[2022-04-17 01:52:47,837][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 32.0, current episode: 3
[2022-04-17 01:52:47,839][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 37.0, current episode: 4
[2022-04-17 01:52:47,841][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 43.0, current episode: 5
[2022-04-17 01:52:47,843][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 420.000000 | iteration_420.pth.tar | 5.000000      | 215.000000    | 43.000000               | 0.018832      | 11416.484953        | 265.499650           | 33.800000   | 5.635601   | 43.000000  | 27.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:47,952][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 20.0, current episode: 1
[2022-04-17 01:52:47,953][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 20.0, current episode: 2
[2022-04-17 01:52:47,954][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 21.0, current episode: 3
[2022-04-17 01:52:47,956][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 24.0, current episode: 4
[2022-04-17 01:52:47,958][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 29.0, current episode: 5
[2022-04-17 01:52:47,960][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 462.000000 | iteration_462.pth.tar | 5.000000      | 145.000000    | 29.000000               | 0.012499      | 11600.920483        | 400.031741           | 22.800000   | 3.429286   | 29.000000  | 20.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:48,069][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 13.0, current episode: 1
[2022-04-17 01:52:48,072][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 15.0, current episode: 2
[2022-04-17 01:52:48,073][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 15.0, current episode: 3
[2022-04-17 01:52:48,074][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 15.0, current episode: 4
[2022-04-17 01:52:48,075][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 16.0, current episode: 5
[2022-04-17 01:52:48,076][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 504.000000 | iteration_504.pth.tar | 5.000000      | 80.000000     | 16.000000               | 0.010035      | 7972.015199         | 498.250950           | 14.800000   | 0.979796   | 16.000000  | 13.000000  |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:48,234][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 200.0, current episode: 1
[2022-04-17 01:52:48,235][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 200.0, current episode: 2
[2022-04-17 01:52:48,237][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 200.0, current episode: 3
[2022-04-17 01:52:48,237][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 200.0, current episode: 4
[2022-04-17 01:52:48,238][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 200.0, current episode: 5
[2022-04-17 01:52:48,239][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 546.000000 | iteration_546.pth.tar | 5.000000      | 1000.000000   | 200.000000              | 0.050045      | 19982.044333        | 99.910222            | 200.000000  | 0.000000   | 200.000000 | 200.000000 |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:48,244][interaction_serial_evaluator.py][line: 245][    INFO] [DI-engine serial pipeline] Current eval_reward: 200.0 is greater than stop_value: 195, so your RL agent is converged, you can refer to 'log/evaluator/evaluator_logger.txt' for details.
[2022-04-17 01:52:54,864][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 0 finish episode, final reward: 200.0, current episode: 1
[2022-04-17 01:52:54,866][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 1 finish episode, final reward: 200.0, current episode: 2
[2022-04-17 01:52:54,866][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 2 finish episode, final reward: 200.0, current episode: 3
[2022-04-17 01:52:54,867][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 3 finish episode, final reward: 200.0, current episode: 4
[2022-04-17 01:52:54,868][interaction_serial_evaluator.py][line: 203][    INFO] [EVALUATOR]env 4 finish episode, final reward: 200.0, current episode: 5
[2022-04-17 01:52:54,869][interaction_serial_evaluator.py][line: 229][    INFO] 
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Name  | train_iter | ckpt_name             | episode_count | envstep_count | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode | reward_mean | reward_std | reward_max | reward_min |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
| Value | 546.000000 | iteration_546.pth.tar | 5.000000      | 1000.000000   | 200.000000              | 5.337433      | 187.355978          | 0.936780             | 200.000000  | 0.000000   | 200.000000 | 200.000000 |
+-------+------------+-----------------------+---------------+---------------+-------------------------+---------------+---------------------+----------------------+-------------+------------+------------+------------+
[2022-04-17 01:52:54,875][interaction_serial_evaluator.py][line: 245][    INFO] [DI-engine serial pipeline] Current eval_reward: 200.0 is greater than stop_value: 195, so your RL agent is converged, you can refer to 'log/evaluator/evaluator_logger.txt' for details.
